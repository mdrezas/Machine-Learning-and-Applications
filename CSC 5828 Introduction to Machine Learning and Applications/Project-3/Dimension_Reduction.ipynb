{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction\n",
    "### ID: eo9232\n",
    "### Name: Md Reza\n",
    "### CSC 5825 - Fall 2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requires packages and libraries\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import random as sparse_random\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_set():\n",
    "    df_train = pd.read_csv(\"/u/mreza6/5825/Data/fashion-mnist_train.csv\")\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_set():\n",
    "    df_test = pd.read_csv(\"/u/mreza6/5825/Data/fashion-mnist_test.csv\")\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_training_set()\n",
    "test_data = load_test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create training and test features where each row uses a vector of dimension 784 with values between 0 (black) and 255 (white) on the gray color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "X_Training_features = train_data.iloc[:,1:785]\n",
    "Y_Training_features = train_data.iloc[:,0]\n",
    "\n",
    "# Test\n",
    "X_Test_features =test_data.iloc[:,1:785]\n",
    "Y_Test_features = test_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling the data before performing SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "scaler = StandardScaler()\n",
    "X_Training_features = scaler.fit_transform(X_Training_features)\n",
    "\n",
    "# Test set \n",
    "X_Test_features= scaler.fit_transform(X_Test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use SVD function to reduce the number of dimensions of training & test data set & report how many components are selected and their variance ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Training Variance: 0.9020745118802875 \n",
      "\n",
      "Training Set Variance Ratio: \n",
      " [0.22057176 0.14395563 0.05458802 0.05116399 0.04069273 0.03012154\n",
      " 0.02750481 0.02325871 0.01694438 0.01309833 0.01161488 0.00963108\n",
      " 0.00890642 0.00856623 0.00743049 0.00730157 0.00657498 0.00632692\n",
      " 0.00623657 0.0058046  0.00515707 0.00511548 0.00472764 0.00453557\n",
      " 0.00438423 0.00416784 0.00395056 0.00393244 0.00378355 0.00374478\n",
      " 0.00368528 0.00353646 0.00336422 0.00330015 0.00329434 0.00319736\n",
      " 0.00305999 0.00293714 0.00289203 0.00280918 0.00271998 0.00265986\n",
      " 0.00255732 0.00253773 0.00245191 0.00243254 0.0023872  0.00228125\n",
      " 0.00223339 0.00215709 0.00212645 0.00209085 0.0020228  0.00201541\n",
      " 0.00199305 0.00195331 0.00191017 0.00185777 0.00181595 0.00178263\n",
      " 0.00175762 0.00173527 0.00170987 0.00167605 0.00161352 0.00157431\n",
      " 0.00154654 0.00150989 0.00148746 0.00146461 0.00144439 0.00143185\n",
      " 0.00142552 0.00140258 0.00137435 0.00137034 0.00134203 0.00130682\n",
      " 0.00129961 0.00127365 0.00125027 0.00122548 0.0012112  0.00119729\n",
      " 0.00118578 0.00114179 0.00113478 0.00112205 0.00110934 0.00110141\n",
      " 0.00109034 0.00107491 0.00106958 0.00104507 0.00102301 0.00100111\n",
      " 0.00099657 0.00098381 0.00096762 0.00095281 0.00095128 0.00093688\n",
      " 0.00093149 0.00091278 0.00090275 0.00089765 0.00088603 0.00087573\n",
      " 0.00086649 0.00085403 0.00084496 0.00083734 0.00082025 0.00081317\n",
      " 0.00080582 0.00079701 0.00079047 0.00078619 0.00077994 0.0007629\n",
      " 0.00075324 0.00074971 0.00074722 0.00073581 0.0007252  0.00071699\n",
      " 0.00070771 0.00069529 0.00068558 0.00068223 0.00067715 0.00066749\n",
      " 0.00066544 0.00064639 0.00062906 0.0006232  0.00061709 0.00061191\n",
      " 0.00060386 0.00059462] \n",
      "\n",
      "Training SVD: (60000, 140) \n",
      "\n",
      "================================================================== \n",
      "\n",
      "Sum of Test Variance: 0.9065898145901772 \n",
      "\n",
      "Test Set Variance Ratio: \n",
      " [0.2203887  0.14443455 0.05489046 0.05045265 0.03989157 0.0301118\n",
      " 0.02799255 0.02262399 0.01738288 0.01395345 0.01195037 0.00968542\n",
      " 0.00956136 0.00883147 0.00818406 0.00722294 0.00699141 0.0064383\n",
      " 0.00621224 0.0061593  0.00561234 0.00514548 0.00483003 0.00466102\n",
      " 0.00456989 0.00437004 0.00421757 0.00403266 0.00391759 0.00378978\n",
      " 0.00363054 0.00353005 0.00336982 0.00334469 0.0033021  0.00316424\n",
      " 0.00309688 0.00302337 0.00293495 0.00275741 0.0027244  0.0026558\n",
      " 0.00259824 0.00257382 0.00248396 0.00241001 0.00234644 0.00226676\n",
      " 0.00223933 0.00217288 0.00215556 0.0020891  0.00204174 0.00201352\n",
      " 0.00195905 0.00190188 0.00188707 0.00181555 0.00178303 0.00173937\n",
      " 0.00171479 0.00170364 0.00169611 0.00168592 0.00162656 0.00160844\n",
      " 0.00156788 0.00154895 0.00151696 0.00149672 0.00148469 0.00144575\n",
      " 0.00141682 0.00137956 0.00136867 0.00133492 0.00132828 0.00130603\n",
      " 0.00129792 0.00127518 0.00127176 0.001238   0.00122969 0.00119955\n",
      " 0.00118826 0.00117664 0.00115546 0.0011199  0.00111341 0.0010855\n",
      " 0.00107696 0.00105116 0.00104351 0.0010377  0.00101231 0.001004\n",
      " 0.00098786 0.00097993 0.00097463 0.00096388 0.00094126 0.0009361\n",
      " 0.00092695 0.00092033 0.00090188 0.00089265 0.00088604 0.00086859\n",
      " 0.00086599 0.00085114 0.00083682 0.00083229 0.00082691 0.00081326\n",
      " 0.00080379 0.00079662 0.00077817 0.00077334 0.00076732 0.00076063\n",
      " 0.00075238 0.00073957 0.00072858 0.00072049 0.00071672 0.00071056\n",
      " 0.00069936 0.00069161 0.00068477 0.00066456 0.00066243 0.00065453\n",
      " 0.00065194 0.00064437 0.0006294  0.0006256  0.00061358 0.00060512\n",
      " 0.00059498 0.00058789] \n",
      "\n",
      "Test SVD: (10000, 140)\n"
     ]
    }
   ],
   "source": [
    "# Training:\n",
    "svd_train = TruncatedSVD(n_components=140).fit(X_Training_features)\n",
    "print(\"Sum of Training Variance:\", svd_train.explained_variance_ratio_.sum(), \"\\n\")\n",
    "print(\"Training Set Variance Ratio:\", \"\\n\", svd_train.explained_variance_ratio_, \"\\n\")\n",
    "SVD_train=svd_train.transform(X_Training_features)\n",
    "print(\"Training SVD:\", SVD_train.shape, \"\\n\")\n",
    "print(\"==================================================================\", \"\\n\")\n",
    "# Test:\n",
    "svd_test= TruncatedSVD(n_components=140).fit(X_Test_features)\n",
    "print(\"Sum of Test Variance:\", svd_test.explained_variance_ratio_.sum(), \"\\n\")\n",
    "print(\"Test Set Variance Ratio:\",  \"\\n\", svd_test.explained_variance_ratio_, \"\\n\")\n",
    "SVD_Test= svd_test.transform(X_Test_features)\n",
    "print(\"Test SVD:\", SVD_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train generative classifiers (Naive Bayes and KNN) and discriminative classifier (multinomial logistic regression) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayesAlgorithm(X,Y):\n",
    "    tmp = GaussianNB()\n",
    "    model=tmp.fit(X,Y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test without SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3178\n"
     ]
    }
   ],
   "source": [
    "NaiveBayes=NaiveBayesAlgorithm(X_Training_features,Y_Training_features)\n",
    "PredNB= NaiveBayes.predict(X_Test_features)\n",
    "print(accuracy_score(Y_Test_features, PredNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5555\n"
     ]
    }
   ],
   "source": [
    "NBA=NaiveBayesAlgorithm(SVD_train,Y_Training_features)\n",
    "p=NBA.predict(SVD_Test)\n",
    "print(accuracy_score(Y_Test_features, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the shape and optimal value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of K: (12000, 140) \n",
      "\n",
      "Optimal K is:  6\n"
     ]
    }
   ],
   "source": [
    "K_T ,K_V, Y_T, Y_V = train_test_split(SVD_train, Y_Training_features, test_size=0.20)\n",
    "optimal_K=0\n",
    "top_acc=0\n",
    "\n",
    "print(\"Shape of K:\", K_V.shape, \"\\n\")\n",
    "for j in range (3,21):\n",
    "    model=KNeighborsClassifier(n_neighbors=j)\n",
    "    model.fit(K_T,Y_T)\n",
    "    pred=model.predict(K_V)\n",
    "    acc=accuracy_score(Y_V, pred)\n",
    "    if(top_acc<acc):\n",
    "        top_acc=acc\n",
    "        optimal_K=j\n",
    "        j=j+1\n",
    "        \n",
    "print('Optimal K is: ',optimal_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict kNN accuracy without SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy without SVD: 0.8602\n"
     ]
    }
   ],
   "source": [
    "KNN_classify= KNeighborsClassifier(n_neighbors=optimal_K)\n",
    "KNN_classify.fit(X_Training_features,Y_Training_features)\n",
    "KNN_pred=KNN_classify.predict(X_Test_features)\n",
    "print('KNN accuracy without SVD:', accuracy_score(Y_Test_features, KNN_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict kNN accuracy with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy with SVD: 0.7241\n"
     ]
    }
   ],
   "source": [
    "KNN_classify.fit(SVD_train,Y_Training_features)\n",
    "KNN_pred_with_svd=KNN_classify.predict(SVD_Test)\n",
    "print('KNN accuracy with SVD:', accuracy_score(Y_Test_features, KNN_pred_with_svd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict MLR accuracy without SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR accuracy without SVD 0.8357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mreza6/.conda/envs/py368nb/lib/python3.6/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "MLR = LogisticRegression(random_state=0, multi_class='multinomial', penalty='none', solver='newton-cg')\n",
    "MLR.fit(X_Training_features,Y_Training_features)\n",
    "MLR_pred=MLR.predict(X_Test_features)\n",
    "print('MLR accuracy without SVD', accuracy_score(Y_Test_features, MLR_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict MLR accuracy with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR accuracy with SVD 0.4808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mreza6/.conda/envs/py368nb/lib/python3.6/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "MLR.fit(SVD_train,Y_Training_features)\n",
    "MLR_with_svd=MLR.predict(SVD_Test)\n",
    "print('MLR accuracy with SVD', accuracy_score(Y_Test_features, MLR_with_svd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of NB, kNN, & MLR with brief descriptions:\n",
    "##### It’s turned out that the kNN has the highest accuracy with/without SVD on the test data set compared to Naive Bayes and Multinomial Logistic Regression with/without SVD. Though from the execution point-view, the kNN took a little longer compared other two classifiers. While Naive Bayes was quick but the accuracy was lower compared to Multinomial Logistic Regression.\n",
    "\n",
    "##### Note: For convergence warning, as suggested, it could have been removed by increasing the number of iterations parameter. But there are possibilities that a large dataset like this generally might not be fitted by a logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
